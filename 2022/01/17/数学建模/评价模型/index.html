<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo/favicon.png"><link rel="icon" href="/img/logo/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="description" content="一个神秘的人"><meta name="author" content="epochxi"><meta name="keywords" content="dramwig"><meta name="description" content="请注意，本文最后更新于2022.2.1，其中一些理解可能已被笔者推翻或废弃。                         本章已经于2022.2.28拆分为四个部分且有所改动，本章不再做更新！            层次分析法(AHP) 层次分析法(Analytic Hierarchy Process,AHP) 这是一种定性和定量相结合的、系统的、层次化的"><meta property="og:type" content="article"><meta property="og:title" content="评价模型"><meta property="og:url" content="http://example.com/2022/01/17/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E6%A8%A1%E5%9E%8B/index.html"><meta property="og:site_name" content="风倾客栈 ✧"><meta property="og:description" content="请注意，本文最后更新于2022.2.1，其中一些理解可能已被笔者推翻或废弃。                         本章已经于2022.2.28拆分为四个部分且有所改动，本章不再做更新！            层次分析法(AHP) 层次分析法(Analytic Hierarchy Process,AHP) 这是一种定性和定量相结合的、系统的、层次化的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s2.loli.net/2022/01/18/oVnIQh5DpORcuqX.png"><meta property="og:image" content="https://s2.loli.net/2022/01/18/f5gYz8SqUmTbahn.png"><meta property="og:image" content="https://s2.loli.net/2022/01/18/BgNZY8JkrPjMKtq.png"><meta property="og:image" content="https://s2.loli.net/2022/01/18/pqFAbi5Ek4SonVa.png"><meta property="og:image" content="https://s2.loli.net/2022/02/01/mO67ZcKR2L34Bau.png"><meta property="article:published_time" content="2022-01-17T01:45:00.000Z"><meta property="article:modified_time" content="2022-02-28T04:42:41.605Z"><meta property="article:author" content="epochxi"><meta property="article:tag" content="TOPSIS"><meta property="article:tag" content="熵权法"><meta property="article:tag" content="层次分析法"><meta property="article:tag" content="模糊综合评价"><meta property="article:tag" content="灰色综合评价"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://s2.loli.net/2022/01/18/oVnIQh5DpORcuqX.png"><title>评价模型 &lt; 风倾客栈 ✧</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/tomorrow-night-bright.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"example.com",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:100,cursorChar:"_",loop:!1},anchorjs:{enable:!1,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hovor",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!1,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:1},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:"fa7203cd147458a2bb6b7c7fc7bd03a3",google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><i class="iconfont icon-home-fill"></i><strong>风倾客栈</strong></a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/categories/">分类</a></li><li class="nav-item"><a class="nav-link" href="/about/">关于</a></li><li class="nav-item"><a class="nav-link" href="/links/">朋友</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="评价模型"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-01-17 09:45" pubdate>2022年1月17日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 15k 字 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">评价模型</h1><div class="markdown-body"><div class="note note-warning"><p>请注意，本文最后更新于2022.2.1，其中一些理解可能已被笔者推翻或废弃。</p></div><div class="note note-warning"><p>本章已经于2022.2.28拆分为四个部分且有所改动，本章不再做更新！</p></div><h1 id="层次分析法ahp">层次分析法(AHP)</h1><p>层次分析法(Analytic Hierarchy Process,AHP)<br>这是一种定性和定量相结合的、系统的、层次化的分析方法。这种方法是在对复杂决策问题的本质上，利用较少的定量信息使<strong>决策的思维过程数学化</strong>，从而为多目标、多准则或无结构特性的决策问题提供简便的决策方法。<strong>是对难以定量的复杂系统进行决策的模型</strong>。</p><p>层次分析法的根本是<strong>打分法</strong>：<u>确定指标</u>，<u>不同方案指标打分</u>，<u>为指标确定权重</u>。用于处理数据未知的评价。</p><p>层次分析法将问题分解为组成因素，并按照因素间关联、影响以及隶属关系将因素按不同的层次聚集组合，形成一个多层次的分析结构模型。从而最终使问题归结为最低层(供决策的方案、措施等)相对于最高层(总目标)的相对重要权值的确定或相对优劣次序的排定。</p><h2 id="基本步骤">基本步骤</h2><h3 id="建立层次模型">建立层次模型</h3><p>思考以下问题：</p><ol type="1"><li>我的的评价目标是什么？</li><li>达到目标有哪些方案？</li><li>★对方案的评价准则或指标是什么？（最好参考引用文献）</li></ol><p>将<u>决策的目标</u>、<u>决策准则</u>(考虑的因素)和<u>决策对象</u>绘制为层次结构图。</p><ol type="1"><li>最高层(目标层)：决策的目的、要解决的问题；</li><li>中间层(准则层或指标层)：考虑的因素、决策的准则；</li><li>最低层(方案层)：决策时的备选方案；</li></ol><p>或仅绘制评价体系(树状图或表格)如下 (要包含<strong>多级指标</strong>)：</p><figure><img src="https://s2.loli.net/2022/01/18/oVnIQh5DpORcuqX.png" srcset="/img/loading.gif" lazyload alt="评价模型"><figcaption aria-hidden="true">评价模型</figcaption></figure><h3 id="构造判断矩阵">构造判断矩阵</h3><p>(成对比较矩阵)</p><p>在确定权重时，只给出定性的结果(就是我认为景色占80%，费用10%等等)，常常不被别人接受，因此采用一致矩阵法，即：</p><ul><li>不把所有因素放在一起比较，而是两两比较</li><li>对此时采用相对尺度，尽可能减少诸因素导致的相互比较的困难，提高准确性</li></ul><p>成对比较矩阵是表示本层所有因素针对上一层某一个因素(准侧或目标)的相对重要性的比较。成对比较矩阵的元素 <span class="math inline">\(a_{ij}\)</span> 表示的是第 <span class="math inline">\(i\)</span> 个因素相对于第 <span class="math inline">\(j\)</span> 个因素的比较结果，这个值使用的是Santy的1-9标度方法给出。</p><p>定义且满足</p><p><span class="math display">\[a_{ij}=i相对j的重要度=\frac{i的重要程度}{j的重要程度}=a_{ik}a_{kj}\]</span></p><p><img src="https://s2.loli.net/2022/01/18/f5gYz8SqUmTbahn.png" srcset="/img/loading.gif" lazyload></p><h3 id="一致性检验">一致性检验</h3><p><span class="math inline">\(\left[\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1 n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2 n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n 1} &amp; a_{n 2} &amp; \cdots &amp; a_{n n} \end{array}\right]\)</span>为一致矩阵的充要条件<span class="math inline">\(\left\{\begin{array}{l} a_{i j}&gt;0 \\ a_{11}=a_{22}=\cdots=a_{n n}=1 \\ {\left[a_{i 1},..., a_{i n}\right]=k_{i}\left[a_{11},..., a_{1 n}\right]} \end{array}\right.\)</span></p><p>对于</p><p>一致阵：则我们自然会取对应于最大特征根 <span class="math inline">\(n\)</span> 的归一化特征向量 <span class="math inline">\(\{w_1,w_2,\cdots,w_n\}\)</span> ，且 <span class="math inline">\(\sum_{i=1}^{n}{w_i=1}\)</span> ， <span class="math inline">\(w_i\)</span> 表示下层第 <span class="math inline">\(i\)</span> 个因素对上层某个因素影响程度的权值。</p><p>非一致阵：用其最大特征根对应的归一化特征向量作为权向量 <span class="math inline">\(W=\{w_1,w_2,\cdots,w_n\}\)</span> ，则 <span class="math inline">\(AW=\lambda W\)</span> ，这样确定权向量的方法称为特征根法；</p><p>定理：</p><ol type="1"><li><span class="math inline">\(n\)</span> 阶一致阵的唯一非零特征根为 <span class="math inline">\(n\)</span></li><li><span class="math inline">\(n\)</span> 阶互反阵 <span class="math inline">\(A\left(a_{i j}&gt;0, a_{i j}=\frac{1}{a_{j i}}, a_{i i}=1\right)\)</span> 最大特征根 <span class="math inline">\(\lambda \geq n\)</span> ，当且仅当 <span class="math inline">\(\lambda=n\)</span> 时， <span class="math inline">\(A\)</span> 为一致矩阵。</li></ol><p><span class="math inline">\(\lambda\)</span> 连续的依赖于 <span class="math inline">\(a_{ij}\)</span> ，则 <span class="math inline">\(\lambda\)</span> 比 <span class="math inline">\(n\)</span> 大的越多， <span class="math inline">\(A\)</span> 的不一致性越严重。用最大特征值对应的特征向量作为影响程度的权向量，其不一致程度越大，引起的判断误差越大。</p><p><strong>第一步：计算一致性指标CI</strong></p><p><span class="math display">\[CI=\frac{\lambda -n}{n-1}\]</span></p><ol type="1"><li><span class="math inline">\(CI=0\)</span> ,有完全的一致性；</li><li><span class="math inline">\(CI\)</span> 接近 <span class="math inline">\(0\)</span> ，有满意的一致性</li><li><span class="math inline">\(CI\)</span> 越大，不一致越严重；</li></ol><p><strong>第二步：查找对应的平均随机一致性指标RI</strong></p><table><colgroup><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"><col style="width:6%"></colgroup><thead><tr class="header"><th style="text-align:center">n</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th><th style="text-align:center">7</th><th style="text-align:center">8</th><th style="text-align:center">9</th><th style="text-align:center">10</th><th style="text-align:center">11</th><th style="text-align:center">12</th><th style="text-align:center">13</th><th style="text-align:center">14</th><th style="text-align:center">15</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">RI</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.52</td><td style="text-align:center">0.89</td><td style="text-align:center">1.12</td><td style="text-align:center">1.26</td><td style="text-align:center">1.36</td><td style="text-align:center">1.41</td><td style="text-align:center">1.46</td><td style="text-align:center">1.49</td><td style="text-align:center">1.52</td><td style="text-align:center">1.54</td><td style="text-align:center">1.56</td><td style="text-align:center">1.58</td><td style="text-align:center">1.59</td></tr></tbody></table><p><span class="math inline">\(RI\)</span> 为统计结果，详细计算方法参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38207837">这里</a></p><p><strong>第三步：计算一致性比例CR</strong></p><p><span class="math display">\[CR=\frac{CI}{RI}\]</span></p><p>如果 <span class="math inline">\(CR &lt; 0.1\)</span>, 则可认为判断矩阵的一致性可以接受；否则需要对判断矩阵进行修正。</p><h3 id="求得权重">求得权重</h3><ol type="1"><li>算术平均法求权重 <span class="math display">\[\omega_{i}=\frac{1}{n} \sum_{j=1}^{n} \frac{a_{i j}}{\sum_{k=1}^{n} a_{k j}}\]</span></li><li>几何平均法求权重 <span class="math display">\[\omega_{i}=\frac{\left(\prod_{j=1}^{n} a_{i j}\right)^{\frac{1}{n}}}{\sum_{k=1}^{n}\left(\prod_{j=1}^{n} a_{k j}\right)^{\frac{1}{n}}}\]</span></li><li>特征值法求权</li></ol><p>特别的：若特征值为n，对应特征向量为<span class="math inline">\(k\left[\frac{1}{a_{11}}, \frac{1}{a_{12}}, \cdots, \frac{1}{a_{1 n}}\right]^{T}\)</span>，且特征向量刚好为矩阵第一列。</p><p>假如我们的判断矩阵一致性可以接受，那么我们可以仿照一致矩阵权重的求法。</p><ol type="1"><li>求出矩阵A的最大特征值以及其对应的特征向量<br></li><li>对求出的特征向量进行归一化即可得到我们的权重( <span class="math inline">\(\sum_{i=1}^{n}{w_i=1}\)</span> )</li></ol><h3 id="填表得结果">填表得结果</h3><figure><img src="https://s2.loli.net/2022/01/18/BgNZY8JkrPjMKtq.png" srcset="/img/loading.gif" lazyload alt="表格"><figcaption aria-hidden="true">表格</figcaption></figure><p>然后相应的加权计算得分即可得到结果。</p><h2 id="一点补充">一点补充</h2><h3 id="详细做法补充">详细做法补充</h3><ol type="1"><li>对于评价指标：<ol type="1"><li>单层评价指标：构造所有指标的两两判断矩阵，得到权值</li><li>多层评价指标：<ol type="1"><li>构造一级指标的两两判断矩阵，得到权值</li><li>构造每个一级指标下的二级指标的两两判断矩阵（每个一级指标一个矩阵），得到权值</li><li>构造每个二级指标下……<br>……</li></ol></li></ol></li><li>对于方案：<ol type="1"><li>对于每个最低级指标构造所有方案的两两判断矩阵，得到权值</li></ol></li></ol><p>示例：</p><p><span class="math display">\[ \left\{\begin{array}{l} 一级指标1\left\{\begin{array}{l} 二级指标1 :a_{11},a_{12},a_{13}\\ 二级指标2 :a_{21},a_{22},a_{23}\\ \end{array}\right.\\ \\ 一级指标2\left\{\begin{array}{l} 二级指标3 :a_{31},a_{32},a_{33}\\ 二级指标4 :a_{41},a_{42},a_{43}\\ 二级指标5 :a_{51},a_{52},a_{53}\\ \end{array}\right.\\ \\ 一级指标3 : a_{61},a_{62},a_{63} \end{array}\right.\\ \]</span></p><h3 id="特征向量含义思考">特征向量含义思考</h3><p>对于矩阵 <span class="math inline">\(A=\left[\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1 n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2 n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n 1} &amp; a_{n 2} &amp; \cdots &amp; a_{n n} \end{array}\right]\)</span> ， <span class="math inline">\(W=\left[\begin{array}{c} w_{1}\\ w_{2}\\ \vdots\\ w_{n} \end{array}\right]\)</span> ，满足 <span class="math inline">\(AW=\lambda A\)</span> 。</p><p>矩阵的本质是变换，把 <span class="math inline">\(A\)</span> 看作对 <span class="math inline">\(m\)</span> 纬空间单位球体进行 <span class="math inline">\(A\)</span> 变换,取 <span class="math inline">\(\lambda\)</span> 最大时的特征向量 <span class="math inline">\(W\)</span> ,即表示为变换后的球体上与变换前方向相同的点中距离原点 <span class="math inline">\(O\)</span> 最远的点（距离原点距离为 <span class="math inline">\(\lambda\)</span> ）所表示的方向向量 。记 <span class="math inline">\(W\)</span> 为可以代表整个变换 <span class="math inline">\(A\)</span> 的线性变换。</p><blockquote><p>特别的对于对称矩阵 <span class="math inline">\(A\)</span> ，变换 <span class="math inline">\(A\)</span> 一定是将球变为椭球，这也是不同特征值对应特征向量一定正交的原因，相同特征值 <span class="math inline">\(\lambda\)</span> 若有多个特征向量 <span class="math inline">\(e_1,e_2,...\)</span>，特征向量的张成空间都是该特征值 <span class="math inline">\(\lambda\)</span> 的特征向量。<span class="math inline">\(\lambda\)</span> 的特征向量。<br>但是，对于非特殊矩阵，几何意义不明确，另外对于正互反矩阵、一致性矩阵的变换性质，笔者并不清楚，所以也仅仅能近似的类比理解到这里了。 (22.1.18)</p></blockquote><h2 id="一些问题">一些问题</h2><ol type="1"><li>评价的决策层不能太多，太多的话n会很大,判断矩阵和一致矩阵差异可能会很大。</li><li>平均随机一致性指标RI的表格中n最多是15。</li></ol><p>所以<strong>当n过大可以分层归纳为多级指标</strong>再构造多个判断矩阵。</p><h1 id="优劣解距离法topsis">优劣解距离法(TOPSIS)</h1><p>TOPSIS法(Technique for Order Preference by Similarity to Ideal Solution)<br>可翻译为逼近理想解排序法，国内常简称为优劣解距离法。是根据有限个评价对象与理想化目标的接近程度进行排序的方法，是在现有的对象中进行相对优劣的评价。</p><p>TOPSIS 法是一种常用的综合评价方法，其能充分利用原始数据的信息，<br>其结果能精确地反映各评价方案之间的差距。</p><h2 id="基本思想">基本思想</h2><p>构造计算评分公式：<span class="math inline">\(\frac{x-min}{max-min}\)</span></p><p>该公式能够良好的对一个指标下的数据进行评分，反应了单个数据在数据范围区间所处的位置。</p><p><strong>拓展</strong>：将其中 <span class="math inline">\(x-min\)</span> 理解为“x与最小点的距离”；<span class="math inline">\(max-x\)</span> 理解为“x于最大值点的距离”即可得到多指标评价公式</p><h2 id="模型步骤">模型步骤</h2><h3 id="数据处理">数据处理</h3><p>对所有的指标构成进行说明</p><p>对于 <span class="math inline">\(m\)</span> 个评价指标横向排列，<span class="math inline">\(n\)</span> 个评价对象纵向排列的矩阵，记为 <span class="math display">\[X_{nm}=\left[\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 m} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n m} \end{array}\right]\]</span></p><h3 id="指标正向化">指标正向化</h3><p><strong>描述</strong>：统一指标类型，将为极小型、中间型、区间型等指标转换为极大值指标</p><p><strong>注</strong>：最好不要出现负数(无法求熵权)</p><ol type="1"><li>极小型指标转换为极大型指标的公式</li></ol><p><span class="math display">\[\bar{x}_{i}=max-x_i或\bar{x}_{i}=\frac{1}{x_i}\]</span> <strong>其中</strong>：<span class="math inline">\(\bar{x}_{i}\)</span>代表转化后的结果</p><ol start="2" type="1"><li>中间型指标转化为极大型指标</li></ol><p><span class="math display">\[\bar{x}_{i}=1-\frac{\left|x_{i}-x_{\text {best }}\right|}{\max \left(\left|X-x_{\text {best }}\right|\right)}\]</span> <strong>其中</strong>：<span class="math inline">\(x_{best}\)</span> 指代的是最好的值</p><ol start="3" type="1"><li>区间型指标转化为极大型指标</li></ol><p><span class="math display">\[ \overline{x_{i}}=\left\{\begin{array}{c} 1-\frac{a-x_{i}}{M}, x_{i}&gt;a \\ 1, \quad a&lt;x_{i}&lt;b \\ 1-\frac{x_{i}-b}{M}, x_{i}&gt;b \end{array} \right. \]</span> <strong>其中</strong>：<span class="math inline">\(M=\max \{a-\min (X), \max (X)-b\}\)</span>即偏离最优区间最远的值; <span class="math inline">\(a\)</span>为下界，<span class="math inline">\(b\)</span>为上界</p><h3 id="标准化处理">标准化处理</h3><p><strong>描述</strong>：消去不同指标量纲的影响</p><p>记标准化矩阵为<span class="math inline">\(Z_{nm}\)</span>,有<span class="math inline">\(Z_{i j}=\frac{x_{i j}}{\sqrt{\sum_{i=1}^{n} x_{i j}^{2}}}\)</span></p><h3 id="计算得分">计算得分</h3><p>定义 <span class="math display">\[ \begin{aligned} Z^{+} &amp;=\left(Z_{1}^{+}, Z_{2}^{+}, \cdots, Z_{m}^{+}\right) \\ &amp;=\left(\max \left\{z_{11}, z_{21}, \cdots, z_{n 1}\right\}, \max \left\{z_{12}, z_{22}, \cdots, z_{n 2}\right\}, \cdots, \max \left\{z_{1 m}, z_{2 m}, \cdots, z_{n m}\right\}\right) \\ Z^{-} &amp;=\left(Z_{1}^{-}, Z_{2}^{-}, \cdots, Z_{m}^{-}\right) \\ &amp;=\left(\min \left\{z_{11}, z_{21}, \cdots, z_{n 1}\right\}, \min \left\{z_{12}, z_{22}, \cdots, z_{n 2}\right\}, \cdots, \min \left\{z_{1 m}, z_{2 m}, \cdots, z_{n m}\right\}\right) \end{aligned} \]</span></p><p>定义第 <span class="math inline">\(i(i=1,2, \cdots, n)\)</span> 个评价对象与最大值的距离 <span class="math inline">\(D_{i}^{+}=\sqrt{\widetilde{w}_{j}\sum_{j=1}^{m}\left(Z_{j}^{+}-z_{i j}\right)^{2}}\)</span><br>定义第 <span class="math inline">\(i(i=1,2, \cdots, n)\)</span> 个评价对象与最小值的距离 <span class="math inline">\(D_{i}^{-}=\sqrt{\widetilde{w}_{j}\sum_{j=1}^{m}\left(Z_{j}^{-}-z_{i j}\right)^{2}}\)</span><br>其中<span class="math inline">\(\widetilde{w}_{j}\)</span>为加权的指标权重，权重默认为1(因为除法，权重单位量对结果无影响)，可以用层次分析法或熵权法确定权重。</p><p>第 <span class="math inline">\(i(i=1,2, \cdots, n)\)</span> 个评价对象未归一化得分<span class="math inline">\(S_{i}=\frac{D_{i}^{-}}{D_{i}^{+}+D_{i}^{-}}\)</span></p><h3 id="结果处理">*结果处理</h3><p>为了比较不同评价模型得到的结果曲线，常常对结果处理。让结果更容易解释，或更好比较。(归一化、标准化)</p><p><span class="math display">\[\widetilde{S_{i}}=\frac{S_{i}}{\sum_{i=1}^{n} S_{i}}\]</span></p><h2 id="熵权法">熵权法</h2><p>熵权法构建系数利用了信息论的知识。简单的说，数据的变异程度（方差）越大，就说明这个指标蕴含的信息量越大，也就越重要。例如：一个指标都是相同的数值，那么这个指标的变异程度（方差）就超级小，也没对最终的判断做出贡献，所以这个指标的权重就可以赋为0。</p><h3 id="模型思想">模型思想</h3><p>记信息量为<span class="math inline">\(I(p)\)</span>，概率为<span class="math inline">\(p\)</span>，有<span class="math inline">\(I(1)=0\)</span>、<span class="math inline">\(I(0^+)=+\infty\)</span>，可以得到一个<span class="math inline">\(I\)</span>与<span class="math inline">\(p\)</span>呈反比的函数图像。</p><p>构造<span class="math inline">\(I(x)=- \ln{p(x)}\)</span>保证<span class="math inline">\(0 \leqslant p(x) \leqslant 1\)</span>。<span class="math inline">\(x\)</span>表示事情发送的情况，<span class="math inline">\(p(x)\)</span>表示事情发生的概率。</p><p>可定义事件<span class="math inline">\(X={x_1,x_2,...,x_n}\)</span>(<span class="math inline">\(x_i\)</span>为发生情况)的信息熵为： <span class="math display">\[H(X)=\sum_{i=1}^{n}\left[p\left(x_{i}\right) I\left(x_{i}\right)\right]=-\sum_{i=1}^{n}\left[p\left(x_{i}\right) \ln \left(p\left(x_{i}\right)\right)\right]\]</span> 信息熵的本质是对信息量的期望值。</p><p><strong>注</strong>：可证当<span class="math inline">\(p(x_1)=p(x_2)=...=p(x_n)=\frac{1}{n}\)</span>时<span class="math inline">\(H(x)\)</span>取得最大值为<span class="math inline">\(\ln n\)</span>。</p><h3 id="基本步骤-1">基本步骤</h3><ol type="1"><li>矩阵正向化和标准化 <span class="math display">\[Z=\left[\begin{array}{cccc} z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1 m} \\ z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{n 1} &amp; z_{n 2} &amp; \cdots &amp; z_{n m} \end{array}\right] \]</span></li><li>对已经正向化和标准化的矩阵，若有负数需再次归一化(另一种标准化) <span class="math display">\[\tilde{z}_{i j}=\frac{x_{i j}-\min \left\{x_{1 j}, x_{2 j}, \cdots, x_{n j}\right\}}{\max \left\{x_{1 j}, x_{2 j}, \cdots, x_{n j}\right\}-\min \left\{x_{1 j}, x_{2 j}, \cdots, x_{n j}\right\}}\]</span></li><li>计算概率矩阵 <span class="math display">\[p_{i j}=\frac{\tilde{z}_{i j}}{\sum_{i=1}^{n} \tilde{z}_{i j}}\]</span></li><li>计算熵值。之所以除以了一个 <span class="math inline">\(ln(n)\)</span> 是为了归一化 <span class="math display">\[e=-\frac{1}{\ln (n)} \times \sum_{i=1}^{n} p_{i} \ln \left(p_{i}\right)\]</span></li><li>构建权重。首先用<span class="math inline">\(1-e_j\)</span>得到信息的效用值，之后因为权重只和为<span class="math inline">\(1\)</span>，在对权重进行归一化即可得到<span class="math inline">\(w_j\)</span>权重 <span class="math display">\[d_j=1-e_j\]</span> <span class="math display">\[w_j=\frac{d_j}{\sum_{j=1}^m d_j}\]</span></li></ol><blockquote><p>熵权法仅仅能反应指标信息量的大小，并不能反应指标的重要程度，所以使用熵权法加权的评价模型得到的结果往往比未加权的结果变化幅度更大，增减趋势更明显。但似乎并不能得到一个客观的评分？ (22.1.21)</p></blockquote><h2 id="更多理解">更多理解</h2><p>对于形如上文 <span class="math inline">\(Z\)</span> 的矩阵，可以将<span class="math inline">\(m\)</span>个评价指标看作<span class="math inline">\(m\)</span>个坐标轴。对于这<span class="math inline">\(m\)</span>个坐标轴的张成空间 <span class="math inline">\(V\)</span> ，每一条横轴（即待评价单元）就可表示为该空间上的一点 <span class="math inline">\(P(x_{i1},x_{i2},...,x_{im})\)</span>。</p><p><span class="math inline">\(Z^{+}(Z_{1}^{+}, Z_{2}^{+}, \cdots, Z_{m}^{+})\)</span> , <span class="math inline">\(Z^{-}(Z_{1}^{-}, Z_{2}^{-}, \cdots, Z_{m}^{-})\)</span> 为包含所有评价点集的最小“<span class="math inline">\(m\)</span>维体”的主对称轴上的两端点。</p><p>对于 <span class="math inline">\(S_{i}=\frac{D_{i}^{-}}{D_{i}^{+}+D_{i}^{-}}=\frac{1}{1+k}\)</span> , <span class="math inline">\(k\)</span> 的含义为点到 <span class="math inline">\(Z^+\)</span> 与 <span class="math inline">\(Z^-\)</span> 的距离比值。形如 <span class="math inline">\(k=\frac{D^+}{D^-}=\frac{dis(X,Z^+)}{dis(X,Z^-)}\)</span> 的式子表示的曲线为<span class="math inline">\(m\)</span>纬的球体。</p><p>以下为2个评价指标时，以 <span class="math inline">\(Z^+(3,2)\)</span> , <span class="math inline">\(Z^-(0,0)\)</span> 为端点，<span class="math inline">\(k=\{2^{-4},2^{-3},...,2^4\}\)</span> 时的函数图像。同一曲线上的点表示的评价结果相同。</p><figure><img src="https://s2.loli.net/2022/01/18/pqFAbi5Ek4SonVa.png" srcset="/img/loading.gif" lazyload alt="二维评价张成空间"><figcaption aria-hidden="true">二维评价张成空间</figcaption></figure><p>如图，可以看出TOPSIS评价对空间进行了扭曲（把<span class="math inline">\(m\)</span>空间非线性压缩为一维评价系）。保留了样本间间距，但损失了原始统计信息。</p><blockquote><p>通过下文解释，所以TOPSIS是高纬的归一化(Normalization)?但下文中说，归一化也是线性放缩。 (22.1.18)</p></blockquote><h2 id="归一化与标准化">*归一化与标准化</h2><p>它们都是指特征工程中的特征线性缩放过程</p><p><strong>作用</strong>：</p><ol type="1"><li>使不同量纲的特征处于同一数值量级，减少方差大的特征的影响，使模型更准确。</li><li>加快学习算法的收敛速度。</li></ol><p><strong>缩放过程可以分为以下几种</strong>：</p><ol type="1"><li>缩放到均值为0，方差为1（Standardization——StandardScaler()）</li><li>缩放到0和1之间（Standardization——MinMaxScaler()）</li><li>缩放到-1和1之间（Standardization——MaxAbsScaler()）</li><li>缩放到0和1之间，保留原始数据的分布（Normalization——Normalizer()）</li></ol><p>1就是常说的z-score归一化，2是min-max归一化。</p><h3 id="归一化normalization">归一化（Normalization）</h3><h4 id="平均归一化mean-normalization">1.平均归一化（Mean Normalization）</h4><p>这种归一化需防止在线学习或在线推理中，新进样本突破原始min或max数据</p><p><span class="math display">\[\frac{X_{i}-\mu }{X_{\max }-X_{\min }}\]</span></p><p><strong>其中</strong>：<span class="math inline">\(X_{min}\)</span>表示对应矩阵中最小的数;<span class="math inline">\(X_{max}\)</span>表示对应矩阵中最大的数</p><h4 id="minmax归一化rescaling">2.minmax归一化（Rescaling）</h4><p>这种归一化需防止在线学习或在线推理中，新进样本突破原始min或max数据 <span class="math display">\[\frac{X_{i}-X_{\min }}{X_{\max }-X_{\min }}\]</span></p><h4 id="非线性归一化non-linear-normlization">3.非线性归一化（Non-Linear Normlization）</h4><p><span class="math display">\[ \begin{array}{c} \lg x \\ \log _{2} x \\ \frac{2}{\pi} \arctan x \end{array} \]</span> 这种归一化需考虑数据分布，进行适当的非线性函数选取。</p><h3 id="标准化standardization">标准化（Standardization）</h3><h4 id="z-score标准化">1.Z-Score标准化</h4><p><span class="math display">\[\frac{X_{i}-\mu}{\sigma}\]</span> 这种标准化需要要求原数据近似高斯分布。</p><p><strong>其中</strong>：<span class="math inline">\(\mu\)</span>表示矩阵均值;<span class="math inline">\(\sigma\)</span>表示矩阵标准差</p><h3 id="中心化centralization">中心化（Centralization）</h3><p>对于中心化，有的地方成为Mean-Subtraction，表示均值-减去（有点日本话的感觉），有的地方称之为Zero-Mean即零均值化，还有的为Zero-Centered这个只可意会很难翻译。但总而言之，他们都是下面这个形式： <span class="math display">\[x_i-\mu\]</span></p><h3 id="区别和用途">区别和用途</h3><p>归一化和标准化的相同点都是对某个特征（column）进行缩放（scaling）而不是对某个样本的特征向量（row）进行缩放。对特征向量进行缩放是毫无意义的。</p><p>归一化和标准化都保持数据分布不变的情况下（两者本质上都只是对数据进行线性变化），对数据进行处理，但是从公式上面还是能够明显看出来，归一化的处理只是和最大值最小值相关，归一化后样本会失去原始的信息；标准化却是和数据的分布相关（均值，方差）。归一化更好保留了样本间间距；标准化的统计意义更强，是数据缩放的首选。</p><table><colgroup><col style="width:57%"><col style="width:42%"></colgroup><thead><tr class="header"><th style="text-align:center">归一化</th><th style="text-align:left"><font size="1">将一列数据“拍扁”到某个固定区间(常为[0,1]),和最大/小值有关</font></th></tr></thead><tbody><tr class="odd"><td style="text-align:center"><strong>标准化</strong></td><td style="text-align:left"><font size="1"><strong>将数据变换为均值0,标准差为1的分布(并非一定正态),缩放和每个点都有关(通过均值+方差体现)</strong></font></td></tr><tr class="even"><td style="text-align:center"><strong>中心化</strong></td><td style="text-align:left"><font size="1"><strong>变量减去均值，可避免异常值和极端值的影响，本质为口个平移过程，平移后数据中心是原点0</strong></font></td></tr></tbody></table><h1 id="灰色关联分析gra">灰色关联分析(GRA)</h1><p>灰色关联度分析（Grey Relation Analysis）<br>可以在一个灰色系统中，衡量某个项目受其他的因素影响的相对强弱。</p><p>两个系统之间的因素，随时间或不同对象而变化的关联性大小的量度，称为关联度。两个因素变化即同步变化程度较高，即可谓二者关联程度较高；反之，则较低。因此，灰色关联分析方法，根据因素之间发展趋势的相似相异程度，为衡量因素间关联程度提供了量化的度量。</p><p>灰色关联分析的<strong>基本思想</strong>是根据<strong>序列曲线几何形状的相似程度</strong>来判断其联系是否紧密。曲线越接近，相应序列之间的关联度就越大，反之就越小。</p><h2 id="基本步骤-2">基本步骤</h2><h3 id="确定分析序列">确定分析序列</h3><ol type="1"><li><strong>母序列(又称参考序列、母指标)</strong></li></ol><p>能反映系统行为特征的数据序列，类似于因变量 <span class="math inline">\(Y\)</span> ，记为</p><p><span class="math display">\[Y=[y_1,y_2,...,y_m]^T\]</span></p><ol start="2" type="1"><li><strong>子序列(又称比较序列、子指标)</strong></li></ol><p>影响系统行为的因素组成的数据序列，类似于因变量 <span class="math inline">\(X\)</span> ，记为</p><p><span class="math display">\[X_{nm}=\left[\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 m} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n m} \end{array}\right]\]</span></p><p><strong>其中</strong>：<span class="math inline">\(n\)</span> 为年份数量，<span class="math inline">\(m\)</span> 为要素的个数</p><h3 id="数据预处理">数据预处理</h3><p>由于不同要素具有不同量纲和数据范围范围，因此我们需要对它们进行归一化(常用均值化)去量纲，将它们统一到近似的范围内，然后重点关注其变化和趋势。</p><p><span class="math display">\[\widetilde{y}_{k}=\frac{y_{k}}{\bar{y_i}}, \quad \bar{y_i}=\frac{1}{n} \sum_{k=1}^{n} y_{k}\]</span></p><p><span class="math display">\[\widetilde{x}_{ki}=\frac{x_{ki}}{\bar{x_{\_i}}}, \quad \bar{x_{\_i}}=\frac{1}{n} \sum_{k=1}^{n} x_{ki}, (i=1,2,...m)\]</span></p><p>注：下文继续用 <span class="math inline">\(X\)</span> 代指预处理后的 <span class="math inline">\(\widetilde{X}\)</span> ， <span class="math inline">\(Y\)</span> 代指 <span class="math inline">\(\widetilde{Y}\)</span>。</p><h3 id="计算灰色关联系数">计算灰色关联系数</h3><p>计算子序列中各个指标与母序列的关联系数</p><p>记 <span class="math display">\[a=\min _{i} \min_{k}\left|x_{0}(k)-x_{i}(k)\right|, \quad b=\max _{i} \max_{k}\left|x_{0}(k)-x_{i}(k)\right|\]</span> 为两极最小差和最大差。</p><p>构造 <span class="math display">\[z_{k,j}=\xi_j(k)=\frac{a+\rho b}{|x_{kj}-y_j|+\rho b}\]</span> 构成 <span class="math inline">\(Z\)</span>。</p><p><strong>其中</strong>：<span class="math inline">\(\rho\)</span>为分辨系数，一般取0.5</p><h3 id="计算关联度">计算关联度</h3><p><span class="math display">\[r_j=\frac{1}{n} \sum_{k=1}^{n} \xi_j(k) = \frac{1}{n} \sum_{k=1}^{n} z_{kj}\]</span></p><h2 id="综合评价">综合评价</h2><h3 id="正向化预处理">正向化预处理</h3><p>记正向化后的数据为： <span class="math display">\[X_{nm}=\left[\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 m} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n m} \end{array}\right]\]</span></p><p><strong>其中</strong>：<span class="math inline">\(n\)</span> 为年份数，<span class="math inline">\(m\)</span> 为指标数</p><h3 id="去除量纲">去除量纲</h3><p>每个数 <span class="math inline">\(x_{ij}\)</span> 除以每一列的平均值 <span class="math inline">\(\bar{x_{\_i}}\)</span> ,记作 <span class="math inline">\(Z\)</span> ，有</p><p><span class="math display">\[z_{ki}=\frac{x_{ki}}{\bar{x_{\_i}}}, \quad \bar{x_{\_i}}=\frac{1}{n} \sum_{k=1}^{n} x_{ki}, (i=1,2,...m)\]</span></p><p><span class="math display">\[Z_{nm}=\left[\begin{array}{cccc} z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1 m} \\ z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{n 1} &amp; z_{n 2} &amp; \cdots &amp; z_{n m} \end{array}\right]\]</span></p><h3 id="构造母序列">构造母序列</h3><p>构造 <span class="math inline">\(Y=[y_1,y_2,...,y_n]^T\)</span> 其中 <span class="math display">\[y_i=max(x_{i1},x_{i2},...,x_{im})\]</span></p><h3 id="计算关联系数">计算关联系数</h3><p>记差值矩阵为 <span class="math inline">\(K\)</span>，有 <span class="math display">\[K_{nm}=\left[\begin{array}{cccc} k_{11} &amp; k_{12} &amp; \cdots &amp; k_{1 m} \\ k_{21} &amp; k_{22} &amp; \cdots &amp; k_{2 m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{k 1} &amp; k_{n 2} &amp; \cdots &amp; k_{n m} \end{array}\right]=\left[\begin{array}{cccc} |x_{11}-y_1| &amp; |x_{12}-y_1| &amp; \cdots &amp; |x_{1 m}-y_1| \\ |x_{21}-y_2| &amp; |x_{22}-y_2| &amp; \cdots &amp; |x_{2 m}-y_2| \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ |x_{n 1}-y_n| &amp; |x_{n 2}-y_n| &amp; \cdots &amp; |x_{n m}-y_n| \end{array}\right]\]</span></p><p>记 <span class="math display">\[a=\min _{i} \min_{j}\left|k_{ij}\right|, \quad b=\max_{i} \max _{k}\left|k_{ij}\right|\]</span> 为两极最小差和最大差。</p><h3 id="计算指标关联度">计算指标关联度</h3><p>记 <span class="math display">\[\xi_{ij}=\frac{a+\rho b}{|k_{ij}|+\rho b}\]</span></p><p><strong>其中</strong>：<span class="math inline">\(\rho\)</span>为分辨系数，一般取0.5</p><p><span class="math display">\[r_j=\frac{1}{n} \sum_{j=1}^{n} \xi_{ij}\]</span></p><p><strong>其中</strong>：<span class="math inline">\(r_j\)</span> 为第 <span class="math inline">\(j\)</span> 个指标的灰色关联度</p><h3 id="计算指标权重">计算指标权重</h3><p><span class="math display">\[w_j=\frac{r_j}{\sum_{k=1}^{m} r_k} (j=1,2,...,m)\]</span></p><h3 id="计算得分-1">计算得分</h3><p><span class="math display">\[S_i= \sum_{j=1}^{m} Z_{ij} \cdot r_j\]</span></p><h3 id="结果归一化">结果归一化</h3><p><span class="math display">\[\tilde{S}_i=\frac{S_i}{\sum_{k=1}^{m} S_k} (i=1,2,\cdots,n)\]</span></p><h2 id="去量纲化方法">*去量纲化方法</h2><h3 id="初值化">初值化</h3><p>顾名思义，就是把这一个序列的数据统一除以最开始的值，由于同一个因素的序列的量级差别不大，所以通过除以初值就能将这些值都整理到1这个量级附近。</p><p><span class="math display">\[f(x(k))=\frac{x(k)}{x(1)}=y(k), \quad x(1) \neq 0\]</span></p><h3 id="均值化">均值化</h3><p>顾名思义，就是把这个序列的数据除以均值，由于数量级大的序列均值比较大，所以除掉以后就能归一化到1的量级附近。</p><p><span class="math display">\[f(x(k))=\frac{x(k)}{\bar{x}}=y(k), \quad \bar{x}=\frac{1}{n} \sum_{k=1}^{n} x(k)\]</span></p><h3 id="百分比变换">百分比变换</h3><p><span class="math display">\[f(x(k))=\frac{x(k)}{\max _{k} x(k)}=y(k)\]</span></p><h3 id="倍数变换">倍数变换</h3><p><span class="math display">\[f(x(k))=\frac{x(k)}{\min _{k} x(k)}=y(k), \quad \min_{k} x(k) \neq 0\]</span></p><h3 id="归一化变换">归一化变换</h3><p><span class="math display">\[f(x(k))=\frac{x(k)}{x_{0}}=y(k)\]</span></p><h3 id="极差最大值变换">极差最大值变换</h3><p><span class="math display">\[f(x(k))=\frac{x(k)-\min _{k} x(k)}{\max_{k} x(k)}=y(k)\]</span></p><h3 id="区间值变换">区间值变换</h3><p><span class="math display">\[f(x(k))=\frac{x(k)-\min _{k} x(k)}{\max_{k} x(k)-\min _{k} x(k)}=y(k)\]</span></p><h1 id="模糊综合评价fuzzy-sets">模糊综合评价(fuzzy sets)</h1><p>模糊综合评价(Fuzzy Comprehension Evaluation Method)又为模糊集合理论(fuzzy sets) 对于难以明确分类，边界不明显的评价指标可才用模糊综合评价，模型关键在于构造的<strong>隶属函数</strong>的准确性和合理性。</p><h2 id="基本步骤-3">基本步骤</h2><h3 id="建立综合评价的因素集">建立综合评价的因素集</h3><p>模糊综合评价指标的构建。<br>因素集是以影响评价对象的各种因素为元素所组成的一个普通集合，通常用 <span class="math inline">\(U\)</span> 表示， <span class="math inline">\(U=(u_1,u_2,\cdots,u_m)\)</span>，其中元素 <span class="math inline">\(u_i\)</span> 代表影响评价对象的第 <span class="math inline">\(i\)</span> 个因素。这些因素，通常都具有不同程度的模糊性。</p><h3 id="建立综合评价的评价集">建立综合评价的评价集</h3><p>评价集是评价者对评价对象可能做出的各种结果所组成的集合，通常用 <span class="math inline">\(V\)</span> 表示， <span class="math inline">\(V=(v_1,v _2,\cdots,v_m)\)</span> ，其中元素 <span class="math inline">\(v_i\)</span> 代表第 <span class="math inline">\(i\)</span> 种评价结果</p><h3 id="构造隶属函数获得评价矩阵">构造隶属函数，获得评价矩阵</h3><p>构造合理美观的隶属函数 <span class="math inline">\(Fuzz(x)\)</span>。</p><p>若因素集U中第i个元素对评价集V中第1个元素的隶属度为 <span class="math inline">\(r_{i1}\)</span> ，则对第 <span class="math inline">\(i\)</span> 个元素单因素使用隶属函数评价的结果用模糊集合表示为：<span class="math inline">\(R_i=(r_{i1},r_{i2},\cdots,r_{in})\)</span>。以 <span class="math inline">\(m\)</span> 个单因素评价集构成 <span class="math inline">\(R_{mn}=[R1,R2,\cdots,Rm]^T\)</span> ，称为模糊综合评价矩阵。</p><h3 id="确定因素权向量">确定因素权向量</h3><p>评价工作中，各因素的重要程度有所不同，为此，给各因素 <span class="math inline">\(u_i\)</span>一个权重 <span class="math inline">\(w_i\)</span> ，各因素的权重集合的模糊集，用 <span class="math inline">\(W\)</span> 表示: <span class="math inline">\(W=(w_1,w_2,\cdots,w_m)\)</span>。</p><h3 id="建立综合评价模型">建立综合评价模型</h3><p>确定单因素评判矩阵 <span class="math inline">\(R\)</span> 和因素权向量 <span class="math inline">\(W\)</span> 之后，通过模糊变化将 <span class="math inline">\(U\)</span>上的模糊向量 <span class="math inline">\(A\)</span> 变为 <span class="math inline">\(V\)</span> 上的模糊向量 <span class="math inline">\(B\)</span>，即 <span class="math display">\[B=W_{1m}\cdot R_{mn}=(b_1,b_2,\cdots,b_n)\]</span></p><p>绘出结果 <span class="math inline">\(B\)</span> 图像，如 <img src="https://pic3.zhimg.com/80/v2-368fd890e8f6524b341b070804782bd2_1440w.jpg" srcset="/img/loading.gif" lazyload></p><h2 id="一些思考">一些思考</h2><p>对于模糊评价，相较于作为一个“综合评价”，我更愿意把模糊综合评价看作为类似于层次分析法的<strong>带有主观意愿的评价</strong>（体现在隶属函数的构造上）；对于模糊集合理论（模糊数学），更愿意看作一种<strong>思想</strong>（隶属程度的思想）。</p><blockquote><p>在2021亚太杯C题的第三四题中我用模糊数学思想，对于“是否属于城市印象范围”构造了这样的“模糊隶属函数”。<span class="math inline">\(Fuz_1(x)=\frac{ℯ^{-(k(x-b))}}{ℯ^{-(k(x-b))}+1}\)</span>、<span class="math inline">\(Fuz_2(x)=ℯ^{\frac{(x-a)^2}{q}}\)</span> 得到了以下的结果。 (22.2.1)</p></blockquote><figure><img src="https://s2.loli.net/2022/02/01/mO67ZcKR2L34Bau.png" srcset="/img/loading.gif" lazyload alt="一些城市相关评价指标评分（隶属）图像"><figcaption aria-hidden="true">一些城市相关评价指标评分（隶属）图像</figcaption></figure><h1 id="参考">参考</h1><p>[1]<bilibili:数学建模学习交流><br>[2]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38207837" class="uri">https://zhuanlan.zhihu.com/p/38207837</a></bilibili:数学建模学习交流></p><p>[3]<a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiashiwendao/p/12130992.html" class="uri">https://www.cnblogs.com/xiashiwendao/p/12130992.html</a><br>[4]<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/TOPSIS%E6%B3%95/3094166?fr=aladdin" class="uri">https://baike.baidu.com/item/TOPSIS%E6%B3%95/3094166?fr=aladdin</a><br>[5]<a target="_blank" rel="noopener" href="https://blog.csdn.net/limiyudianzi/article/details/103410150" class="uri">https://blog.csdn.net/limiyudianzi/article/details/103410150</a><br>[6]<bilibili:数学建模学习交流><br>[7]<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20467170" class="uri">https://www.zhihu.com/question/20467170</a><br>[8]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364338617" class="uri">https://zhuanlan.zhihu.com/p/364338617</a></bilibili:数学建模学习交流></p><p>[9]<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%E6%B3%95/8602076?fr=aladdin" class="uri">https://baike.baidu.com/item/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%E6%B3%95/8602076?fr=aladdin</a><br>[10]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/149479206" class="uri">https://zhuanlan.zhihu.com/p/149479206</a><br>[11]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266959639" class="uri">https://zhuanlan.zhihu.com/p/266959639</a></p><p>[12]<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95/2162444" class="uri">https://baike.baidu.com/item/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95/2162444</a><br>[13]<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32666445" class="uri">https://zhuanlan.zhihu.com/p/32666445</a></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/TOPSIS/">TOPSIS</a> <a class="hover-with-bg" href="/tags/%E7%86%B5%E6%9D%83%E6%B3%95/">熵权法</a> <a class="hover-with-bg" href="/tags/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/">层次分析法</a> <a class="hover-with-bg" href="/tags/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7/">模糊综合评价</a> <a class="hover-with-bg" href="/tags/%E7%81%B0%E8%89%B2%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7/">灰色综合评价</a></div></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js",(function(){var e=Object.assign({appId:"t9jCQpAs0bonvdH69rBoVs08-gzGzoHsz",appKey:"IVwl94UT4wnOPWUgwn4AfOCE",path:"window.location.pathname",placeholder:"世界的小小漂泊者呀，把你的足迹留在我的文字里吧…",avatar:"robohash",avatarForce:!0,meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",admin_email_hash:"393669798@qq.com",enableQQ:!0,emojiCDN:"//i0.hdslb.com/bfs/emote/",emojiMaps:{tv_doge:"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_亲亲":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再见":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_发怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_发财":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可爱":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_呕吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_坏笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尴尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_惊吓":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"}},{el:"#valine",path:window.location.pathname});new Valine(e),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{Fluid.plugins.initFancyBox("#valine .vcontent img:not(.vemoji)")})}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content">&copy;<script type="text/javascript">var myDate=(new Date).getFullYear();document.write(myDate)</script><a target="_blank" rel="noopener" href="https://dramwig.github.io/" title="风倾 | 船动莲开" targe="_black">风倾</a> | 船动莲开</div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},loader:{load:["ui/lazy"]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),o=document.createTextNode("");t.parentNode.replaceChild(o,t),n.start={node:o,delim:"",n:0},n.end={node:o,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script><script defer>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?fa7203cd147458a2bb6b7c7fc7bd03a3";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="/js/boot.js"></script></body></html>