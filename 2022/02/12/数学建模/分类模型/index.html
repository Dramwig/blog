<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/logo/favicon.png"><link rel="icon" href="/img/logo/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="description" content="一个神秘的人"><meta name="author" content="epochxi"><meta name="keywords" content="dramwig"><meta name="description" content="判别分析是在已知研究对象分成若干类型并已经取得各种类型的一批已知样本的观测数据，在此基础上根据某些准则建立判别式，然后对未知类型的样品进行判别分析。 0-1回归 即采用逻辑回归(logistic regressio) 把观测数据作为因变量对虚拟变量(0,1)进行回归，将y看作事件0发生的概率。 模型构造 构造线性概率模型 \[y_{i}&#x3D;\beta_{0}+\beta_{1} x_{1"><meta property="og:type" content="article"><meta property="og:title" content="分类模型"><meta property="og:url" content="http://example.com/2022/02/12/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/index.html"><meta property="og:site_name" content="风倾客栈 ✧"><meta property="og:description" content="判别分析是在已知研究对象分成若干类型并已经取得各种类型的一批已知样本的观测数据，在此基础上根据某些准则建立判别式，然后对未知类型的样品进行判别分析。 0-1回归 即采用逻辑回归(logistic regressio) 把观测数据作为因变量对虚拟变量(0,1)进行回归，将y看作事件0发生的概率。 模型构造 构造线性概率模型 \[y_{i}&#x3D;\beta_{0}+\beta_{1} x_{1"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s2.loli.net/2022/02/12/YVulZUi73kohRfN.png"><meta property="og:image" content="https://s2.loli.net/2022/02/12/Eusi4l2NkGBMZj1.png"><meta property="og:image" content="https://s2.loli.net/2022/02/12/6gjoFQa8Hy5NWR9.png"><meta property="article:published_time" content="2022-02-12T06:53:00.000Z"><meta property="article:modified_time" content="2022-02-12T11:16:37.099Z"><meta property="article:author" content="epochxi"><meta property="article:tag" content="0-1回归"><meta property="article:tag" content="Fisher线性判别"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://s2.loli.net/2022/02/12/YVulZUi73kohRfN.png"><title>分类模型 &lt; 风倾客栈 ✧</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/tomorrow-night-bright.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"example.com",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:100,cursorChar:"_",loop:!1},anchorjs:{enable:!1,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hovor",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!1,image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:1},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:"fa7203cd147458a2bb6b7c7fc7bd03a3",google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname"}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><i class="iconfont icon-home-fill"></i><strong>风倾客栈</strong></a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/categories/">分类</a></li><li class="nav-item"><a class="nav-link" href="/about/">关于</a></li><li class="nav-item"><a class="nav-link" href="/links/">朋友</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="分类模型"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-02-12 14:53" pubdate>2022年2月12日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.1k 字 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div><div class="scroll-down-bar"><i class="iconfont icon-arrowdown"></i></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">分类模型</h1><div class="markdown-body"><p>判别分析是在已知研究对象分成若干类型并已经取得各种类型的一批已知样本的观测数据，在此基础上根据某些准则建立判别式，然后对未知类型的样品进行判别分析。</p><h1 id="回归">0-1回归</h1><p>即采用逻辑回归(logistic regressio)<br>把观测数据作为因变量对虚拟变量(0,1)进行回归，将y看作事件0发生的概率。</p><h2 id="模型构造">模型构造</h2><h3 id="构造线性概率模型">构造线性概率模型</h3><p><span class="math display">\[y_{i}=\beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\cdots+\beta_{k} x_{k i}+\mu_{i}\]</span></p><p>㝍成向量乘积形式: <span class="math inline">\(y_{i}=\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}+u_{i}(i=1,2, \cdots, n)\)</span></p><h3 id="构造两点分布">构造两点分布</h3><p><span class="math display">\[\left\{\begin{array}{l} P(y=1 \mid \boldsymbol{x})=F(\boldsymbol{x}, \boldsymbol{\beta}) \\ P(y=0 \mid \boldsymbol{x})=1-F(\boldsymbol{x}, \boldsymbol{\beta}) \end{array} \text { 注: 一般 } F(\boldsymbol{x}, \boldsymbol{\beta})=S\left(\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}\right)\right.\]</span></p><h3 id="构造连接函数">构造连接函数</h3><p>要使 <span class="math inline">\(F(\boldsymbol{x},\boldsymbol{\beta})\)</span> 为值域 <span class="math inline">\([0,1]\)</span> 上的函数。构造以下函数：</p><ol type="1"><li><span class="math inline">\(F(\boldsymbol{x},\boldsymbol{\beta})\)</span> 取正态分布累计密度函数<span class="math inline">\((cdf)\)</span> <span class="math display">\[\begin{array}{c} F(\boldsymbol{x}, \boldsymbol{\beta})=\Phi\left(\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}\right)=\int_{-\infty}^{\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^{2}}{2}} d t \\ \color{Red} (\text { probit 回归 }) \end{array}\]</span></li><li><span class="math inline">\(F(\boldsymbol{x},\boldsymbol{\beta})\)</span> 取<span class="math inline">\(Sigmoid\)</span>函数 <span class="math display">\[\begin{aligned} F(\boldsymbol{x}, \boldsymbol{\beta})=&amp; S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)=\frac{\exp \left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)}{1+\exp \left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)} \\ &amp; \color{Red} (\text{ logistic回归 }) \end{aligned}\]</span></li></ol><h3 id="参数求解">参数求解</h3><p>对于非线性模型采用极大似然估计法(MLE)估计： <span class="math display">\[f\left(y_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\beta}\right)=\left\{\begin{array}{ll} S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right) &amp; , y_{i}=1 \\ 1-S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right) &amp; , y_{i}=0 \end{array}\right.\]</span> 即 <span class="math display">\[f\left(y_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\beta}\right)=\left[S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)\right]^{y_{i}}\left[1-S\left(\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}\right)\right]^{1-y_{i}}\]</span> 取对数 <span class="math display">\[\ln f\left(y_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\beta}\right)=y_{i} \ln \left[S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)\right]+\left(1-y_{i}\right) \ln \left[1-S\left(\boldsymbol{x}_{i}^{\prime} \boldsymbol{\beta}\right)\right]\]</span> 得到样本对数似然函数 <span class="math display">\[\left.\ln L(\boldsymbol{\beta} \mid \boldsymbol{y}, \boldsymbol{x})=\sum_{i=1}^{n} y_{i} \ln \left[S\left(\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}\right)\right]+\sum_{i=1}^{n}\left(1-y_{i}\right) \ln \left[1-S\left(\boldsymbol{x}_{\boldsymbol{i}}^{\prime} \boldsymbol{\beta}\right)\right]\right.\]</span> 求这个函数的最大化。</p><h3 id="求准确率">求准确率</h3><p>把数据分为<strong>训练组</strong>和<strong>测试组</strong>，一般取80%和20%，多次随机抽选训练组和测试组，对模型求一个平均的准确率(交叉验证)。</p><p><img src="https://s2.loli.net/2022/02/12/YVulZUi73kohRfN.png" srcset="/img/loading.gif" lazyload></p><h2 id="思考">思考</h2><p>对于这个模型 <span class="math inline">\(\hat{y}\)</span> 理解为为事件“<span class="math inline">\(y=1\)</span>”发生的概率</p><p><span class="math display">\[\hat{y}_{i}=P\left(y_{i}=1 \mid \boldsymbol{x}\right)=S\left(\boldsymbol{x}_{i}^{\prime} \hat{\boldsymbol{\beta}}\right)=\frac{\exp \left(\boldsymbol{x}_{i}^{\prime} \hat{\boldsymbol{\beta}}\right)}{1+\exp \left(\boldsymbol{x}_{i}^{\prime} \hat{\boldsymbol{\beta}}\right)}=\frac{e^{\widehat{\beta}_{0}+\widehat{\beta}_{1} x_{1 i}+\widehat{\beta}_{2} x_{2 i}+\cdots+\widehat{\beta}_{k} x_{k i}}}{1+e^{\widehat{\beta}_{0}+\widehat{\beta}_{1} x_{1 i}+\widehat{\beta}_{2} x_{2 i}+\cdots+\widehat{\beta}_{k} x_{k i}}}\]</span></p><p>对于 <span class="math inline">\(S\left(\boldsymbol{x}_{i}^{\prime} \hat{\boldsymbol{\beta}}\right)=\frac{1}{2}\)</span> 即</p><p><span class="math display">\[{\widehat{\beta}_{0}+\widehat{\beta}_{1} x_{1 i}+\widehat{\beta}_{2} x_{2 i}+\cdots+\widehat{\beta}_{k} x_{k i}}=0\]</span></p><p>可以看作一个超平面（ <span class="math inline">\(k=2\)</span>时是直线 ）。该分类的本质就是找到一个“曲线”把 '<span class="math inline">\(y=0\)</span>' 和 '<span class="math inline">\(y=1\)</span>' 分割到曲线的两侧。</p><h1 id="fisher线性判别分析">Fisher线性判别分析</h1><p>线性判别式分析(Linear Discriminant Analysis, LDA)<br>也叫做Fisher线性判别(Fisher Linear Discriminant ,FLD)</p><p>基本思想：选取适当的投影方向<strong>将高维数据投影到低维空间上</strong>使得投影后各样本类<strong>内离差</strong>平方和尽可能小，而使各样本类间的离差平方和尽可能大<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://www.jianshu.com/p/cd5ac0b95c03
">[2]</span></a></sup>。即，使得在子空间上有最佳的可分离性。</p><figure><img src="https://s2.loli.net/2022/02/12/Eusi4l2NkGBMZj1.png" srcset="/img/loading.gif" lazyload alt="LDA.png"><figcaption aria-hidden="true">LDA.png</figcaption></figure><h2 id="一维压缩的fisher判别模型">一维压缩的Fisher判别模型</h2><p>Fisher判别可以将 <span class="math inline">\(C\)</span> 维指标压缩到 <span class="math inline">\([1,C-1]\)</span> 纬空间。这里只解释将样本压缩到一维的情况。</p><p>设样本数据集为 <span class="math inline">\(\{\boldsymbol{D_1},\boldsymbol{D_2}\}\)</span> (二维指标)， <span class="math inline">\(\boldsymbol{X_i}\)</span> 是第 <span class="math inline">\(i\)</span> 类样本的集合 ，<span class="math inline">\(X_i\)</span> 是第 <span class="math inline">\(i\)</span> 类样本的集合，第 <span class="math inline">\(i\)</span> 个集合样本是数目是 <span class="math inline">\(N_i\)</span> ，<span class="math inline">\(x=(d_1,d_2)\)</span>。最佳的向量称为 <span class="math inline">\(\boldsymbol{w}\)</span><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://zhuanlan.zhihu.com/p/137968371
">[4]</span></a></sup>。</p><p>记 <span class="math inline">\(\mu_i\)</span> 为第i类样本质心，为 <span class="math display">\[\mu_{i}=\frac{1}{N_{i}} \sum_{x \in \boldsymbol{X_{i}}} x\]</span></p><p><span class="math inline">\(x\)</span> 到 <span class="math inline">\(\boldsymbol{w}\)</span> 投影后的样本点均值为</p><p><span class="math display">\[\tilde{\mu}_{i}=\frac{1}{N_{i}} \sum_{x \in \boldsymbol{X}_{i}} y=\frac{1}{N_{i}} \sum_{x \in \boldsymbol{X}_{i}} \boldsymbol{w}^{T} x=\boldsymbol{w}^{T} \mu_{i}\]</span></p><p><strong>类间方差最大化</strong>，那么就是要让 <span class="math inline">\(J(\boldsymbol{w})\)</span> 最大</p><p><span class="math display">\[J(\boldsymbol{w})=\left|\tilde{\mu}_{1}-\tilde{\mu}_{2}\right|=\left|\boldsymbol{w}^{T}\left(\mu_{1}-\mu_{2}\right)\right|\]</span></p><p>但是只满足这一个条件并不行，两个类达到了类间方差最大化的条件，但是可能轴上的投影两个类重合的部分太多，无法进行有效的区分。</p><p><strong>类内方差最小化</strong></p><p>对于类内方差，使用另外一个值<strong>散列值（scatter）</strong>，计算方法如果下，其实就是投影值与中心之间的方差和。目的就是使这个值最小。</p><p><span class="math display">\[\tilde{s_{i}}^{2}=\sum_{y \in \boldsymbol{Y}_{i}}\left(y-\tilde{\mu}_{i}\right)^{2},(y=\boldsymbol{w}^Tx)\]</span></p><p>最后，我们结合这两个条件</p><p><span class="math display">\[J(\boldsymbol{w})=\frac{\left|\tilde{\mu_{1}}-\tilde{\mu_{2}}\right|}{\tilde{s_{1}}^{2}+\tilde{s_{2}}^{2}}\]</span></p><p>我们要做的就是让 <span class="math inline">\(J(\boldsymbol{w})\)</span> 最大。<a target="_blank" rel="noopener" href="https://blog.csdn.net/antkillerfarm/article/details/80880221">推导过程</a></p><p>结论：</p><p><span class="math display">\[S_{i}=\sum_{x \in \omega_{i}}\left(x-\mu_{i}\right)\left(x-\mu_{i}\right)^{T}\]</span></p><p><span class="math display">\[S_{W}=S_{1}+S_{2}\]</span></p><p><span class="math display">\[\boldsymbol{w}=S_{W}^{-1}\left(\mu_{1}-\mu_{2}\right)\]</span></p><h2 id="思考-1">思考</h2><p><img src="https://img-blog.csdnimg.cn/20190516120230815.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BpbmFwcGxlTWk=,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload></p><p><strong>主成分分析(PCA)</strong>:是找到方差尽可能大的维度，使得信息尽可能都保存，不考虑样本的可分离性，不具备预测功能。</p><p><strong>线性判别分析(LAD)</strong>:是找到一个低维的空间，投影后，使得可分离性最佳(保留信息趋少)，投影后可进行判别以及对新的样本进行预测<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://blog.csdn.net/PinappleMi/article/details/90261680
">[3]</span></a></sup>。</p><p>相较于压缩至一维空间，压缩到二维平面或超平面上常常更能分离出不同的样本。 <img src="https://s2.loli.net/2022/02/12/6gjoFQa8Hy5NWR9.png" srcset="/img/loading.gif" lazyload></p><h1 id="参考">参考</h1><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>清风数学建模 <a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>https://www.jianshu.com/p/cd5ac0b95c03 <a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>https://blog.csdn.net/PinappleMi/article/details/90261680 <a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>https://zhuanlan.zhihu.com/p/137968371 <a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/0-1%E5%9B%9E%E5%BD%92/">0-1回归</a> <a class="hover-with-bg" href="/tags/Fisher%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB/">Fisher线性判别</a></div></div><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/02/13/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">聚类分析</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/02/08/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"><span class="hidden-mobile">回归分析</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js",(function(){var e=Object.assign({appId:"t9jCQpAs0bonvdH69rBoVs08-gzGzoHsz",appKey:"IVwl94UT4wnOPWUgwn4AfOCE",path:"window.location.pathname",placeholder:"世界的小小漂泊者呀，把你的足迹留在我的文字里吧…",avatar:"robohash",avatarForce:!0,meta:["nick","mail","link"],requiredFields:[],pageSize:10,lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:"",admin_email_hash:"393669798@qq.com",enableQQ:!0,emojiCDN:"//i0.hdslb.com/bfs/emote/",emojiMaps:{tv_doge:"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_亲亲":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再见":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_发怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_发财":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可爱":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_呕吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_坏笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尴尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_惊吓":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"}},{el:"#valine",path:window.location.pathname});new Valine(e),Fluid.utils.waitElementVisible("#valine .vcontent",()=>{Fluid.plugins.initFancyBox("#valine .vcontent img:not(.vemoji)")})}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content">&copy;<script type="text/javascript">var myDate=(new Date).getFullYear();document.write(myDate)</script><a target="_blank" rel="noopener" href="https://dramwig.github.io/" title="风倾 | 船动莲开" targe="_black">风倾</a> | 船动莲开</div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},loader:{load:["ui/lazy"]},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),o=document.createTextNode("");t.parentNode.replaceChild(o,t),n.start={node:o,delim:"",n:0},n.end={node:o,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}}</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script><script defer>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?fa7203cd147458a2bb6b7c7fc7bd03a3";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="/js/boot.js"></script></body></html>